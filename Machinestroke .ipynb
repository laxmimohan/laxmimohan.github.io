{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this project we will build a Machine Learning model to predict whether an indiviudal will have a stroke.  The data used in this project can be found on kaggle at the following link: https://www.kaggle.com/asaumya/healthcare-data#train_2v.csv\n",
    "\n",
    "# In this notebook, we build and implement our Machine Learning model.  To view our initial data analysis, please see the notebook titled \"Data_Analysis.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>age</th>\n",
       "      <th>average_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>children</td>\n",
       "      <td>smokes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>other</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>58.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>other</td>\n",
       "      <td>smokes</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>other</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>other</td>\n",
       "      <td>smokes</td>\n",
       "      <td>14.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>19.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hypertension  heart_disease ever_married work_type   smoking_status   age  \\\n",
       "0             0              0           No  children           smokes   3.0   \n",
       "1             1              0          Yes     other     never smoked  58.0   \n",
       "2             0              0           No     other           smokes   8.0   \n",
       "3             0              0          Yes     other  formerly smoked  70.0   \n",
       "4             0              0           No     other           smokes  14.0   \n",
       "\n",
       "   average_glucose_level   bmi  stroke  \n",
       "0                  95.12  18.0       0  \n",
       "1                  87.96  39.2       0  \n",
       "2                 110.89  17.6       0  \n",
       "3                  69.04  35.9       0  \n",
       "4                 161.28  19.1       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define file path to our data\n",
    "stoke_data_relevant_features_and_label_file_path = os.path.join(\"..\", \"Data\", \"stroke_data_relevant_features_and_label.csv\")\n",
    "\n",
    "# Create dataframe from local csv file \n",
    "stroke_data_relevant_features_and_label = pd.read_csv(stoke_data_relevant_features_and_label_file_path)\n",
    "\n",
    "# Previe dataframe\n",
    "stroke_data_relevant_features_and_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to one hot encode our categorical columns, so we will convert each 0 to \"No,\" and each 1 to \"Yes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people of age 0: 0\n",
      "Number of people of age 1: 34\n"
     ]
    }
   ],
   "source": [
    "# Before we replace 0 and 1 with \"no\" and \"yes\",\n",
    "# we should check to see if either of these numbers are present in the age column\n",
    "number_of_people_age_0 = len(stroke_data_relevant_features_and_label[stroke_data_relevant_features_and_label[\"age\"] == 0])\n",
    "number_of_people_age_1 = len(stroke_data_relevant_features_and_label[stroke_data_relevant_features_and_label[\"age\"] == 1])\n",
    "\n",
    "print(f\"Number of people of age 0: {number_of_people_age_0}\")\n",
    "print(f\"Number of people of age 1: {number_of_people_age_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we replace all values of 0 and 1 with \"No\" and \"Yes,\"\n",
    "# we are going to replace ages of 1 with a value of \"Yes\"\n",
    "# We will also replace the binary data in the stroke column with strings.\n",
    "# We will therefore make copies of these rows to put back in the dataframe after our initial replacement\n",
    "\n",
    "copy_of_data = pd.DataFrame()\n",
    "\n",
    "# copy_of_data[\"age\"] = stroke_data_relevant_features_and_label[\"age\"]\n",
    "# copy_of_data[\"stroke\"] = stroke_data_relevant_features_and_label[\"stroke\"]\n",
    "\n",
    "copy_of_data_age = [stroke_data_relevant_features_and_label[\"age\"]]\n",
    "copy_of_data_stroke = [stroke_data_relevant_features_and_label[\"stroke\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3795: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n",
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3795: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>age</th>\n",
       "      <th>average_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>children</td>\n",
       "      <td>smokes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>other</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>58.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>other</td>\n",
       "      <td>smokes</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>other</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>other</td>\n",
       "      <td>smokes</td>\n",
       "      <td>14.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>19.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hypertension  heart_disease ever_married work_type   smoking_status   age  \\\n",
       "0             0              0           No  children           smokes   3.0   \n",
       "1             1              0          Yes     other     never smoked  58.0   \n",
       "2             0              0           No     other           smokes   8.0   \n",
       "3             0              0          Yes     other  formerly smoked  70.0   \n",
       "4             0              0           No     other           smokes  14.0   \n",
       "\n",
       "   average_glucose_level   bmi  stroke  \n",
       "0                  95.12  18.0       0  \n",
       "1                  87.96  39.2       0  \n",
       "2                 110.89  17.6       0  \n",
       "3                  69.04  35.9       0  \n",
       "4                 161.28  19.1       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace each 0 with \"No,\" and each 1 with \"Yes.\"\n",
    "stroke_data_relevant_features_and_label[[\"hypertension\", \"heart_disease\"]].replace(0, \"No\", inplace=True)\n",
    "stroke_data_relevant_features_and_label[[\"hypertension\", \"heart_disease\"]].replace(1, \"Yes\", inplace=True)\n",
    "\n",
    "# Preview dataframe after converting binary data to strings\n",
    "stroke_data_relevant_features_and_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people of age 1: 34\n"
     ]
    }
   ],
   "source": [
    "# Check to see if either if the values of 1 in the age column were changed\n",
    "number_of_people_age_1 = len(stroke_data_relevant_features_and_label[stroke_data_relevant_features_and_label[\"age\"] == 1])\n",
    "\n",
    "print(f\"Number of people of age 1: {number_of_people_age_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace the values in the post-replacement age and stroke columns with the original values\n",
    "# stroke_data_relevant_features_and_label[\"age\"] = copy_of_data_age\n",
    "# stroke_data_relevant_features_and_label[\"stroke\"] = copy_of_data_stroke\n",
    "\n",
    "# # Preview dataframe to confirm values in stroke column were fixed\n",
    "# stroke_data_relevant_features_and_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    39339\n",
      "1     4061\n",
      "Name: hypertension, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0    41338\n",
      "1     2062\n",
      "Name: heart_disease, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Confirm binary data proplerly converted\n",
    "print(stroke_data_relevant_features_and_label[\"hypertension\"].value_counts())\n",
    "print(100*\"-\")\n",
    "print(stroke_data_relevant_features_and_label[\"heart_disease\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>average_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "      <th>hypertension_0</th>\n",
       "      <th>hypertension_1</th>\n",
       "      <th>heart_disease_0</th>\n",
       "      <th>heart_disease_1</th>\n",
       "      <th>ever_married_No</th>\n",
       "      <th>ever_married_Yes</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>work_type_other</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>95.12</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>87.96</td>\n",
       "      <td>39.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>110.89</td>\n",
       "      <td>17.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70.0</td>\n",
       "      <td>69.04</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>161.28</td>\n",
       "      <td>19.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  average_glucose_level   bmi  stroke  hypertension_0  hypertension_1  \\\n",
       "0   3.0                  95.12  18.0       0               1               0   \n",
       "1  58.0                  87.96  39.2       0               0               1   \n",
       "2   8.0                 110.89  17.6       0               1               0   \n",
       "3  70.0                  69.04  35.9       0               1               0   \n",
       "4  14.0                 161.28  19.1       0               1               0   \n",
       "\n",
       "   heart_disease_0  heart_disease_1  ever_married_No  ever_married_Yes  \\\n",
       "0                1                0                1                 0   \n",
       "1                1                0                0                 1   \n",
       "2                1                0                1                 0   \n",
       "3                1                0                0                 1   \n",
       "4                1                0                1                 0   \n",
       "\n",
       "   work_type_Self-employed  work_type_children  work_type_other  \\\n",
       "0                        0                   1                0   \n",
       "1                        0                   0                1   \n",
       "2                        0                   0                1   \n",
       "3                        0                   0                1   \n",
       "4                        0                   0                1   \n",
       "\n",
       "   smoking_status_formerly smoked  smoking_status_never smoked  \\\n",
       "0                               0                            0   \n",
       "1                               0                            1   \n",
       "2                               0                            0   \n",
       "3                               1                            0   \n",
       "4                               0                            0   \n",
       "\n",
       "   smoking_status_smokes  \n",
       "0                      1  \n",
       "1                      0  \n",
       "2                      1  \n",
       "3                      0  \n",
       "4                      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform data to one hot encoded data\n",
    "machine_ready_stroke_data = pd.get_dummies(stroke_data_relevant_features_and_label, columns=[\"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"smoking_status\"])\n",
    "machine_ready_stroke_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Maching Learning algorithms will we try out\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_blobs\n",
    "\n",
    "# X, y = make_blobs(centers=2, random_state=42)\n",
    "\n",
    "# print(f\"Labels: {y[:10]}\")\n",
    "# print(f\"Data: {X[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our features and label\n",
    "X = np.array(machine_ready_stroke_data.drop([\"stroke\"], axis=1))\n",
    "y = np.array(machine_ready_stroke_data[\"stroke\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our features and labels, but the data is still imbalanced.  We will try employing SMOTE to handle this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following section, we will try running several loops to see what the effect of changing several parameters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SMOTE to handle the imbalanced data issue\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import tree to use the DecisionTreeClassifier() algorithm\n",
    "# from sklearn import  tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the SMOTE parameter sampling_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the multi parameter to ovr yields an accuracy of 0.7774113767518549\n",
      "Setting the multi parameter to multinomial yields an accuracy of 0.7768406366922442\n",
      "Setting the multi parameter to auto yields an accuracy of 0.7795675058659395\n"
     ]
    }
   ],
   "source": [
    "multilist = ['ovr', 'multinomial', 'auto']\n",
    "           \n",
    "for multi in multilist:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver= \"newton-cg\" , multi_class=multi)\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the multi parameter to {multi} yields an accuracy of {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the solver parameter to newton-cg yields an accuracy of 0.7772845456274969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the solver parameter to lbfgs yields an accuracy of 0.7730357029615067\n",
      "Setting the solver parameter to liblinear yields an accuracy of 0.7815968038556662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the solver parameter to sag yields an accuracy of 0.7748747542646965\n",
      "Setting the solver parameter to saga yields an accuracy of 0.7766503900057073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    " solverlist = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "                \n",
    "        \n",
    "for solver in solverlist:\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver=solver)\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the solver parameter to {solver} yields an accuracy of {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 2 yields an accuracy of 0.7809626482338766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 4 yields an accuracy of 0.7764601433191706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 6 yields an accuracy of 0.780391908174266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 8 yields an accuracy of 0.7810894793582345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 10 yields an accuracy of 0.7808358171095187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 12 yields an accuracy of 0.7768406366922442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 14 yields an accuracy of 0.7792504280550447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the random_state to 16 yields an accuracy of 0.7781723634980025\n",
      "Setting the random_state to 18 yields an accuracy of 0.7805821548608028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "random_statelist = np.arange(2, 20, 2)\n",
    "                  \n",
    "    \n",
    "for random_state in random_statelist:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=random_state)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver= \"lbfgs\")\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the random_state to {random_state} yields an accuracy of {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.5 yields an accuracy of 0.7931951505670708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.55 yields an accuracy of 0.7911746896760521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.6000000000000001 yields an accuracy of 0.7901451825780906\n",
      "Setting the sampling_strategy_argument to 0.6500000000000001 yields an accuracy of 0.7819965870307167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.7000000000000002 yields an accuracy of 0.7797791580400276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.7500000000000002 yields an accuracy of 0.7783588093322606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.8000000000000003 yields an accuracy of 0.7783209490288098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.8500000000000003 yields an accuracy of 0.774177183080728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy_argument to 0.9000000000000004 yields an accuracy of 0.7785118863846866\n",
      "Setting the sampling_strategy_argument to 0.9500000000000004 yields an accuracy of 0.7814812586486974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "sampling_strategy_arguments = np.arange(0.5,1 , .05)\n",
    "                  \n",
    "    \n",
    "for sampling_strategy_argument in sampling_strategy_arguments:\n",
    "    smote = SMOTE(sampling_strategy_argument, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver= \"lbfgs\")\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the sampling_strategy_argument to {sampling_strategy_argument} yields an accuracy of {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7898518844405338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7818543799772468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7828847481021394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7811075355323143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7758440881241038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7777284545627497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.777894411855511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAXMI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the sampling_strategy parameter to saga yields an accuracy of 0.7835268636062812\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ecd5099635e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Setting the sampling_strategy parameter to {solver} yields an accuracy of {score}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "sampling_strategy_arguments = np.arange(0.6, 1, 0.05)\n",
    "\n",
    "for sampling_strategy_argument in sampling_strategy_arguments:\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the sampling_strategy parameter to {solver} yields an accuracy of {score}\")\n",
    "\n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the SMOTE parameter k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "k_neighbors_arguments = np.arange(1, 102, 10)\n",
    "\n",
    "for k_neighbors_argument in k_neighbors_arguments:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=k_neighbors_argument)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegressionClassifier()\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the k_neighbor parameter to {k_neighbors_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_split parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the train_test_split parameter test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "test_size_arguments = np.arange(0.05, 0.5, 0.05)\n",
    "\n",
    "for test_size_argument in test_size_arguments:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = test_size_argument, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegressionClassifier()\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the train_test_split parameter to {test_size_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the train_test_split parameter random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "random_state_arguments = np.arange(1, 10, 1)\n",
    "\n",
    "for random_state_argument in random_state_arguments:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = test_size_argument, random_state=random_state_argument)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegressionClassifier()\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the random_state parameter to {random_state_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier() parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the DecisionTreeClassifier() parameter max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "max_depth_arguments = np.arange(1, 102, 10)\n",
    "\n",
    "for max_depth_argument in max_depth_arguments:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegressionClassifier(max_depth=max_depth_argument, max_leaf_nodes=10)\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the max_depth parameter to {max_depth_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the cell below we examine how accuracy changes when adjusting the DecisionTreeClassifier() parameter max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of arguments to iteratively try out\n",
    "max_nodes_arguments = np.arange(2, 153, 10)\n",
    "\n",
    "for max_nodes_argument in max_nodes_arguments:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the max_node parameter to {max_nodes_argument} yields an accuracy of {score}\")\n",
    "\n",
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have a better idea of what impact each parameter does, we will try one final test below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "# Split the data into training and testing sets\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "\n",
    "# Create, fit, and score the decision tree classifier\n",
    "classifier = tree.DecisionTreeClassifier(max_depth=100, max_leaf_nodes=100)\n",
    "classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "score = classifier.score(X_smote_test, y_smote_test)\n",
    "\n",
    "print(score, \"\\n\")\n",
    "print(classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "feature_names = [\"age\",\n",
    "                 \"average_glucose_levels\",\n",
    "                 \"bmi\",\n",
    "                 \"hypertension_0\",\n",
    "                 \"hypertension_1\",\n",
    "                 \"heart_disease_0\",\n",
    "                 \"heart_disease_1\",\n",
    "                 \"ever_married_No\",\n",
    "                 \"ever_married_Yes\",\n",
    "                 \"work_type_Self-employed\",\n",
    "                 \"work_type_children\",\n",
    "                 \"work_type_other\",\n",
    "                 \"smoking_status_formerly_smoked\",\n",
    "                 \"smoking_status_never_smoked\",\n",
    "                 \"smoking_status_smokes\"\n",
    "                ]\n",
    "class_names=[\"did_not_have_a_stroke\", \"had_a_stroke\"]\n",
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file=None, \n",
    "                      feature_names=feature_names,\n",
    "                      class_names=class_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-21-c26f9f9aa2d9>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-c26f9f9aa2d9>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    print(\"2. split, scale, SMOTE\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(\"2. split, scale, SMOTE\")\n",
    "scores = []\n",
    "for sampling_strategy_argument in sampling_strategy_arguments:\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create scaler for features and label\n",
    "    X_train_scaler = StandardScaler().fit(X_train)\n",
    "    X_test_scaler = StandardScaler().fit(X_test)\n",
    "#     y_scaler = StandardScaler().fit(y_train)\n",
    "    \n",
    "    # Scale features and labels\n",
    "    X_train_scaled = X_train_scaler.transform(X_train)\n",
    "    X_test_scaled = X_test_scaler.transform(X_test)\n",
    "#     y_train_scaled = y_scaler.transform(y_train)\n",
    "\n",
    "    # Use SMOTE to handle class imbalance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=8)\n",
    "    X_train_scaled_SMOTE, y_train_SMOTE = smote.fit_sample(X_train_scaled, y_train.ravel())\n",
    "    y_train_SMOTE = y_train_SMOTE.reshape(-1,1)\n",
    "\n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = tree.DecisionTreeClassifier(max_depth=10, max_leaf_nodes=10)\n",
    "    classifier = classifier.fit(X=X_train_scaled_SMOTE, y=y_train_SMOTE)\n",
    "    score = classifier.score(X_test_scaled, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the sampling_strategy parameter to {sampling_strategy_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "average_accuracy = sum(scores)/len(scores)\n",
    "print(f\"Average accuracy: {average_accuracy}\")\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "    print(\"2. split, scale, SMOTE\")\n",
    "    scores = []\n",
    "    for sampling_strategy_argument in sampling_strategy_arguments:\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create scaler for features and label\n",
    "    X_train_scaler = StandardScaler().fit(X_train)\n",
    "    X_test_scaler = StandardScaler().fit(X_test)\n",
    "#     y_scaler = StandardScaler().fit(y_train)\n",
    "    \n",
    "    # Scale features and labels\n",
    "    X_train_scaled = X_train_scaler.transform(X_train)\n",
    "    X_test_scaled = X_test_scaler.transform(X_test)\n",
    "#     y_train_scaled = y_scaler.transform(y_train)\n",
    "\n",
    "    # Use SMOTE to handle class imbalance\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy_argument, k_neighbors=8)\n",
    "    X_train_scaled_SMOTE, y_train_SMOTE = smote.fit_sample(X_train_scaled, y_train.ravel())\n",
    "    y_train_SMOTE = y_train_SMOTE.reshape(-1,1)\n",
    "\n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegressionClassifier()\n",
    "    classifier = classifier.fit(X=X_train_scaled_SMOTE, y=y_train_SMOTE)\n",
    "    score = classifier.score(X_test_scaled, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the sampling_strategy parameter to {sampling_strategy_argument} yields an accuracy of {score}\")\n",
    "    \n",
    "average_accuracy = sum(scores)/len(scores)\n",
    "print(f\"Average accuracy: {average_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the multi parameter to ovr yields an accuracy of 0.7774113767518549\n",
      "Setting the multi parameter to multinomial yields an accuracy of 0.7768406366922442\n",
      "Setting the multi parameter to auto yields an accuracy of 0.7795675058659395\n"
     ]
    }
   ],
   "source": [
    "multilist = ['ovr', 'multinomial', 'auto']\n",
    "           \n",
    "for multi in multilist:\n",
    "    smote = SMOTE(sampling_strategy=0.85, k_neighbors=4)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y.ravel())\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size = 0.2, random_state=3)\n",
    "    \n",
    "    # Create, fit, and score the decision tree classifier\n",
    "    classifier = LogisticRegression(random_state=0, solver= \"newton-cg\" , multi_class=multi)\n",
    "    classifier = classifier.fit(X=X_smote_train, y= y_smote_train)\n",
    "    score = classifier.score(X_smote_test, y_smote_test)\n",
    "    \n",
    "    # Print a list of accuracies based on the current argument\n",
    "    print(f\"Setting the multi parameter to {multi} yields an accuracy of {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create scale for features and label\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "# Scale features and labels\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# models.append((\"LR\", LogisticRegression()))\n",
    "# models.append((\"CART\", DecisionTreeClassifier()))\n",
    "# models.append((\"CART\", RandomForestClassifier()))\n",
    "# models.append((\"SVM\", SVC()))\n",
    "# models.append((\"NB\", GaussianNB()))\n",
    "\n",
    "# from sklearn import model_selection\n",
    "\n",
    "# # Evaluate each model in turn\n",
    "# results = []\n",
    "# names = []\n",
    "\n",
    "# for name, model in models:\n",
    "#     kfold = model_selection.KFold(n_splits=10, random_state=42)\n",
    "#     cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print(f\"{name}: {cv_results.mean()}, {cv_results.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at this\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "\n",
    "# decision_tree_data = tree.export_graphviz(\n",
    "#   classifier,\n",
    "#   out_file=None,\n",
    "#   feature_names=[\"age\",\n",
    "#                  \"average_glucose_levels\",\n",
    "#                  \"bmi\",\n",
    "#                  \"hypertension_0\",\n",
    "#                  \"hypertension_1\",\n",
    "#                  \"heart_disease_0\",\n",
    "#                  \"heart_disease_1\",\n",
    "#                  \"ever_married_No\",\n",
    "#                  \"ever_married_Yes\",\n",
    "#                  \"work_type_Self-employed\",\n",
    "#                  \"work_type_children\",\n",
    "#                  \"work_type_other\",\n",
    "#                  \"smoking_status_formerly_smoked\",\n",
    "#                  \"smoking_status_never_smoked\",\n",
    "#                  \"smoking_status_smokes\"\n",
    "#                 ],\n",
    "#     class_names=[\"did_not_have_a_stroke\", \"had_a_stroke\"],\n",
    "#     filled=True,\n",
    "#     rounded=False\n",
    "# )\n",
    "\n",
    "# graph = graphviz.Source(decision_tree_data)\n",
    "# #graph\n",
    "\n",
    "# #graph[size=\"7.75,10.25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "decision_tree_data = tree.export_graphviz(\n",
    "  classifier,\n",
    "  out_file=None,\n",
    "  feature_names=[\"age\",\n",
    "                 \"average_glucose_levels\",\n",
    "                 \"bmi\",\n",
    "                 \"hypertension_0\",\n",
    "                 \"hypertension_1\",\n",
    "                 \"heart_disease_0\",\n",
    "                 \"heart_disease_1\",\n",
    "                 \"ever_married_No\",\n",
    "                 \"ever_married_Yes\",\n",
    "                 \"work_type_Self-employed\",\n",
    "                 \"work_type_children\",\n",
    "                 \"work_type_other\",\n",
    "                 \"smoking_status_formerly_smoked\",\n",
    "                 \"smoking_status_never_smoked\",\n",
    "                 \"smoking_status_smokes\"\n",
    "                ],\n",
    "    class_names=[\"did_not_have_a_stroke\", \"had_a_stroke\"],\n",
    "    filled=True,\n",
    "    rounded=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "graph = graphviz.Source(decision_tree_data)\n",
    "graph\n",
    "\n",
    "#graph[size=\"7.75,10.25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tree.export_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.render(format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing both classes\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=1)\n",
    "rf = rf.fit(X_train, np.array(y_train))\n",
    "rf.score(X_test, np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
